---
title: ARM64内存管理十九：内存压缩算法
date: 2019-09-30
categories:
- linux-memory
tags:
- compact_pages
---

## 前沿

### 往篇回顾

>在上一篇中，主要学习了lru及lruvec机制；lru用于内存回收，lruvec则用于辅助lru，避免频繁加锁访问lru

* lru链表有5种不同类型，分别装5种不同状态的page

* lruvec则是根据lru的操作进行分类的

     1. 将不处于lru链表的新页放入到lru链表中

     2. 将非活动lru链表中的页移动到非活动lru链表尾部
     
     3. 将处于活动lru链表尾部的页移动到非活动lru链表
     
     4. 将处于非活动lru链表的页移动到活动lru链表头

### 内存压缩基本知识

	 
### 本篇主要内容

>有了lru做铺垫，本篇主要分析内存回收之内存压缩的基本过程

## 代码解析

### try_to_compact_pages

``` c++
/*
 * 遍历zonelist中每个zone：
 * 1. 判断此zone的内存压缩是否需要推迟 compaction_deferred()
 * 2. 调用compact_zone_order->compact_zone()进行内存压缩
 * 		a. 如果压缩后能分配到内存，则停止对其余zone的内存压缩
 * 		b. 如果压缩不成功则需要进行收尾处理 defer_compaction(), 减少后续对此zone的压缩频率，因为很可能还是无法成功
 * compact_considered: 内存压缩推迟计数器;compact_defer_shift: 内存压缩推迟阀值; compact_order_failed: 内存压缩失败最大order值
 */
unsigned long try_to_compact_pages(gfp_t gfp_mask, unsigned int order,
			int alloc_flags, const struct alloc_context *ac,
			enum migrate_mode mode, int *contended)
{
	/* 用于检查是否能使用文件系统的IO操作 */
	int may_enter_fs = gfp_mask & __GFP_FS;
	/* 用于检查是否能使用磁盘IO操作 */
	int may_perform_io = gfp_mask & __GFP_IO;
	struct zoneref *z;
	struct zone *zone;
	int rc = COMPACT_DEFERRED;
	int all_zones_contended = COMPACT_CONTENDED_LOCK; /* init for &= op */

	*contended = COMPACT_CONTENDED_NONE;

	/* 如果order = 0或者不允许使用文件系统IO和磁盘IO，则跳过本次压缩，因为不使用IO有可能导致死锁 */
	if (!order || !may_enter_fs || !may_perform_io)
		return COMPACT_SKIPPED;

	trace_mm_compaction_try_to_compact_pages(order, gfp_mask, mode);

	/* Compact each zone in the list */
	/* 遍历zonelist中的每一个zone */
	for_each_zone_zonelist_nodemask(zone, z, ac->zonelist, ac->high_zoneidx,
								ac->nodemask) {
		int status;
		int zone_contended;
		/* 检查是否跳过此次压缩，两种情况：
		 * 1. zone->compact_considered < (1<<zone->compact_defer_shift)，由于压缩耗费资源，要限制压缩的频次
		 * 2. order>=zone->compact_order_failed
		 */
		if (compaction_deferred(zone, order))
			continue;
		/* 对遍历到的zone进行内存压缩 */
		status = compact_zone_order(zone, order, gfp_mask, mode,
				&zone_contended, alloc_flags,
				ac->classzone_idx);
		rc = max(status, rc);
		/*
		 * It takes at least one zone that wasn't lock contended
		 * to clear all_zones_contended.
		 */
		all_zones_contended &= zone_contended;

		/* If a normal allocation would succeed, stop compacting */
		/* 判断压缩后此zone分配1 << order个页框后剩余的页框数量, 是否 大于 此zone的低阀值 + 保留的页框数量 */
		if (zone_watermark_ok(zone, order, low_wmark_pages(zone),
					ac->classzone_idx, alloc_flags)) {
			/* 当zone内存满足low阀值 + (1 << order) + 保留的内存 时，则将compact_order_failed设置为本次压缩的order + 1 
             * 因为这里还不确定内存压缩是否成功了，只是此zone的剩余页框满足了要求
             */
			compaction_defer_reset(zone, order, false);
			/* 异步情况下需要被调度时会设置 */
			if (zone_contended == COMPACT_CONTENDED_SCHED)
				*contended = COMPACT_CONTENDED_SCHED;
			/* 当zone中空闲内存达到 low阀值 + (1 << order) + 保留的内存 时，就不对下面的zone进行内存压缩了 */
			goto break_loop;
		}
		/* 本zone压缩失败，进行收尾处理：
		 * 如果是同步压缩或者轻同步压缩，则增加推迟计数器阀值zone->compact_defer_shift, zone->compact_considered=0 */
		if (mode != MIGRATE_ASYNC && status == COMPACT_COMPLETE) {
			/*
			 * We think that allocation won't succeed in this zone
			 * so we defer compaction there. If it ends up
			 * succeeding after all, it will be reset.
			 */
			defer_compaction(zone, order);
		}

		/*
		 * We might have stopped compacting due to need_resched() in
		 * async compaction, or due to a fatal signal detected. In that
		 * case do not try further zones and signal need_resched()
		 * contention.
		 */
		if ((zone_contended == COMPACT_CONTENDED_SCHED)
					|| fatal_signal_pending(current)) {
			*contended = COMPACT_CONTENDED_SCHED;
			goto break_loop;
		}

		continue;
break_loop:
		/*
		 * We might not have tried all the zones, so  be conservative
		 * and assume they are not all lock contended.
		 */
		all_zones_contended = 0;
		break;
	}

	/*
	 * If at least one zone wasn't deferred or skipped, we report if all
	 * zones that were tried were lock contended.
	 */
	if (rc > COMPACT_SKIPPED && all_zones_contended)
		*contended = COMPACT_CONTENDED_LOCK;

	return rc;
}
```



### kswapd_run

``` c++
/*
 * This kswapd start function will be called by init and node-hot-add.
 * On node-hot-add, kswapd will moved to proper cpus if cpus are hot-added.
 */
int kswapd_run(int nid)
{
	/* 获取内存节点对应的pg_data_t指针 */
	pg_data_t *pgdat = NODE_DATA(nid);
	int ret = 0;

	if (pgdat->kswapd)
		return 0;
	/* 启动kswapd线程 */
	pgdat->kswapd = kthread_run(kswapd, pgdat, "kswapd%d", nid);
	if (IS_ERR(pgdat->kswapd)) {
		/* failure at boot is fatal */
		BUG_ON(system_state == SYSTEM_BOOTING);
		pr_err("Failed to start kswapd on node %d\n", nid);
		ret = PTR_ERR(pgdat->kswapd);
		pgdat->kswapd = NULL;
	}
	return ret;
}
```

### kswapd

``` c++
/*
 * The background pageout daemon, started as a kernel thread
 * from the init process.
 *
 * This basically trickles out pages so that we have _some_
 * free memory available even if there is no other activity
 * that frees anything up. This is needed for things like routing
 * etc, where we otherwise might have all activity going on in
 * asynchronous contexts that cannot page things out.
 *
 * If there are applications that are active memory-allocators
 * (most normal use), this basically shouldn't matter.
 */
static int kswapd(void *p)
{
	unsigned long order, new_order;
	unsigned balanced_order;
	int classzone_idx, new_classzone_idx;
	int balanced_classzone_idx;
	pg_data_t *pgdat = (pg_data_t*)p;
	struct task_struct *tsk = current;

	struct reclaim_state reclaim_state = {
		.reclaimed_slab = 0,
	};
	const struct cpumask *cpumask = cpumask_of_node(pgdat->node_id);

	lockdep_set_current_reclaim_state(GFP_KERNEL);

	if (!cpumask_empty(cpumask))
		set_cpus_allowed_ptr(tsk, cpumask);
	current->reclaim_state = &reclaim_state;

	/*
	 * Tell the memory management that we're a "memory allocator",
	 * and that if we need more memory we should get access to it
	 * regardless (see "__alloc_pages()"). "kswapd" should
	 * never get caught in the normal page freeing logic.
	 *
	 * (Kswapd normally doesn't need memory anyway, but sometimes
	 * you need a small amount of memory in order to be able to
	 * page out something else, and this flag essentially protects
	 * us from recursively trying to free more memory as we're
	 * trying to free the first piece of memory in the first place).
	 */
	tsk->flags |= PF_MEMALLOC | PF_SWAPWRITE | PF_KSWAPD;
	set_freezable();

	order = new_order = 0;
	balanced_order = 0;
	classzone_idx = new_classzone_idx = pgdat->nr_zones - 1;
	balanced_classzone_idx = classzone_idx;
	for ( ; ; ) {
		bool ret;

		/*
		 * If the last balance_pgdat was unsuccessful it's unlikely a
		 * new request of a similar or harder type will succeed soon
		 * so consider going to sleep on the basis we reclaimed at
		 */
		if (balanced_classzone_idx >= new_classzone_idx &&
					balanced_order == new_order) {
			new_order = pgdat->kswapd_max_order;
			new_classzone_idx = pgdat->classzone_idx;
			pgdat->kswapd_max_order =  0;
			pgdat->classzone_idx = pgdat->nr_zones - 1;
		}

		if (order < new_order || classzone_idx > new_classzone_idx) {
			/*
			 * Don't sleep if someone wants a larger 'order'
			 * allocation or has tigher zone constraints
			 */
			order = new_order;
			classzone_idx = new_classzone_idx;
		} else {
			kswapd_try_to_sleep(pgdat, balanced_order,
						balanced_classzone_idx);
			order = pgdat->kswapd_max_order;
			classzone_idx = pgdat->classzone_idx;
			new_order = order;
			new_classzone_idx = classzone_idx;
			pgdat->kswapd_max_order = 0;
			pgdat->classzone_idx = pgdat->nr_zones - 1;
		}

		ret = try_to_freeze();
		if (kthread_should_stop())
			break;

		/*
		 * We can speed up thawing tasks if we don't call balance_pgdat
		 * after returning from the refrigerator
		 */
		if (!ret) {
			trace_mm_vmscan_kswapd_wake(pgdat->node_id, order);
			balanced_classzone_idx = classzone_idx;
			balanced_order = balance_pgdat(pgdat, order,
						&balanced_classzone_idx);
		}
	}

	tsk->flags &= ~(PF_MEMALLOC | PF_SWAPWRITE | PF_KSWAPD);
	current->reclaim_state = NULL;
	lockdep_clear_current_reclaim_state();

	return 0;
}
```

### kswapd_init

``` c++

```### kswapd_init

``` c++

```### kswapd_init

``` c++

```
