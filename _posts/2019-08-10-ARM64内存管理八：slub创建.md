---
title: ARM64内存管理七：slub机制
date: 2019-08-10
categories:
- linux-memory
tags:
- mm,slab
---


## 前沿

### 往篇回顾

在上一篇中，我们正式进入slub系统的学习，主要分析了slub系统初始化的过程，关键函数是`start_kernel()->mm_init()->kmem_cache_init()`，在该函数中依次完成了以下动作：

* 完成slub管理框架内存申请，即两个关键slub："kmem_cache", "kmem_cache_node";

* 使用`bootstrap`函数刷新各个内存node节点中struct page->slab_cache指针；

* 在`create_kmalloc_caches`中，完成size_index和kmem_caches映射，并使用`create_kmalloc_cache`初始化kmem_caches结构体数组；


### 本篇主要内容

>本篇主要分析slub分配器申请过程


## 代码分析

### kmem_cache_create

``` c++
/*
 * kmem_cache_create - Create a cache.
 * @name: 用于/proc/slabinfo文件中显示此高速缓冲的字符串
 * @size: 要创建的slub中每个对象的大小
 * @align: 对象对齐偏移量
 * @flags: 对应slab的标志
 * @ctor: 构建对象构造函数
 * 
 * 函数成功时返回指向cache的指针，失败时返回NULL.
 * 中断中不允许调用，但是调用过程中可以被中断.
 * 当针对cache的新的页框分配成功时运行ctor构造函数.
 *
 */
struct kmem_cache *
kmem_cache_create(const char *name, size_t size, size_t align,
		  unsigned long flags, void (*ctor)(void *))
{
	struct kmem_cache *s;
	const char *cache_name;
	int err;

	get_online_cpus();
	get_online_mems();
	memcg_get_cache_ids();

	mutex_lock(&slab_mutex);

	err = kmem_cache_sanity_check(name, size);
	if (err) {
		s = NULL;	/* suppress uninit var warning */
		goto out_unlock;
	}

	flags &= CACHE_CREATE_MASK;
	/* 
	 * 调用find_mergeable查找是否有能复用的kmem_cache,全局变量slab_unmerge被置位时不允许复用：
	 * list_for_each_entry_reverse会遍历slab_cache全局链表，检查以下几项
	 * 1. 两者的size要足够接近，差距小于sizeof(void *)；且现有的要大些，当然新创建的size要经过L1对齐
	 * 2. flags标志
	 * 3. 两个kemem_cache对齐要一致
	 *
	 * 如果找到合适的复用kmem_cache，则kmem_cache->refcount++，refcount反映kmem_cache的复用次数
     * 最后调用sysfs_slab_alias在sysfs中添加别名
	 */
	s = __kmem_cache_alias(name, size, align, flags, ctor);
	if (s)
		goto out_unlock;
	/* 
	 * 使用GFP_KERNEL模式申请内存，将形参name的内容copy到这段新申请的内存中
	 * 如果name本身在rdata数据区，则不复制
	 */
	cache_name = kstrdup_const(name, GFP_KERNEL);
	if (!cache_name) {
		err = -ENOMEM;
		goto out_unlock;
	}
	/* 
	 * 将主要参数配置到slab描述符，然后将得到的描述符加入slab_caches全局链表中，其中：
	 * 1. kmem_cache_zalloc：使用GFP_KERNEL, 为kmem_cache结构体申请内存
     * 2. 初始化kmem_cache结构体数据
	 * 3. __kmem_cache_create： 创建slub描述符的核心：对齐操作、计算需要的页面、对象数目、对slab着色等，后面会进一步分析
	 * 
	 */
	s = do_kmem_cache_create(cache_name, size, size,
				 calculate_alignment(flags, align, size),
				 flags, ctor, NULL, NULL);
	if (IS_ERR(s)) {
		err = PTR_ERR(s);
		kfree_const(cache_name);
	}

out_unlock:
	mutex_unlock(&slab_mutex);

	memcg_put_cache_ids();
	put_online_mems();
	put_online_cpus();

	if (err) {
		if (flags & SLAB_PANIC)
			panic("kmem_cache_create: Failed to create slab '%s'. Error %d\n",
				name, err);
		else {
			printk(KERN_WARNING "kmem_cache_create(%s) failed with error %d",
				name, err);
			dump_stack();
		}
		return NULL;
	}
	return s;
}
```

### __kmem_cache_create

``` c++
/*
 * 初始化slub结构
 */
int __kmem_cache_create(struct kmem_cache *s, unsigned long flags)
{
	int err;

	err = kmem_cache_open(s, flags);
	if (err)
		return err;

	/* Mutex is not taken during early boot */
	if (slab_state <= UP)
		return 0;

	memcg_propagate_slab_attrs(s);
    /* 将kmem_cache添加到sysfs */
	err = sysfs_slab_add(s);
	if (err)
    /*如果出错则销毁slub*/
		kmem_cache_close(s);

	return err;
}
```

### kmem_cache_open

``` c++
static int kmem_cache_open(struct kmem_cache *s, unsigned long flags)
{
	s->flags = kmem_cache_flags(s->size, flags, s->name, s->ctor);
	s->reserved = 0;

	if (need_reserve_slab_rcu && (s->flags & SLAB_DESTROY_BY_RCU))
		s->reserved = sizeof(struct rcu_head);

	if (!calculate_sizes(s, -1))
		goto error;
	if (disable_higher_order_debug) {
		/*
		 * Disable debugging flags that store metadata if the min slab
		 * order increased.
		 */
		if (get_order(s->size) > get_order(s->object_size)) {
			s->flags &= ~DEBUG_METADATA_FLAGS;
			s->offset = 0;
			if (!calculate_sizes(s, -1))
				goto error;
		}
	}

#if defined(CONFIG_HAVE_CMPXCHG_DOUBLE) && \
    defined(CONFIG_HAVE_ALIGNED_STRUCT_PAGE)
	if (system_has_cmpxchg_double() && (s->flags & SLAB_DEBUG_FLAGS) == 0)
		/* Enable fast mode */
		s->flags |= __CMPXCHG_DOUBLE;
#endif

	/*
	 * The larger the object size is, the more pages we want on the partial
	 * list to avoid pounding the page allocator excessively.
	 */
	set_min_partial(s, ilog2(s->size) / 2);

	/*
	 * cpu_partial determined the maximum number of objects kept in the
	 * per cpu partial lists of a processor.
	 *
	 * Per cpu partial lists mainly contain slabs that just have one
	 * object freed. If they are used for allocation then they can be
	 * filled up again with minimal effort. The slab will never hit the
	 * per node partial lists and therefore no locking will be required.
	 *
	 * This setting also determines
	 *
	 * A) The number of objects from per cpu partial slabs dumped to the
	 *    per node list when we reach the limit.
	 * B) The number of objects in cpu partial slabs to extract from the
	 *    per node list when we run out of per cpu objects. We only fetch
	 *    50% to keep some capacity around for frees.
	 */
	if (!kmem_cache_has_cpu_partial(s))
		s->cpu_partial = 0;
	else if (s->size >= PAGE_SIZE)
		s->cpu_partial = 2;
	else if (s->size >= 1024)
		s->cpu_partial = 6;
	else if (s->size >= 256)
		s->cpu_partial = 13;
	else
		s->cpu_partial = 30;

#ifdef CONFIG_NUMA
	s->remote_node_defrag_ratio = 1000;
#endif
	if (!init_kmem_cache_nodes(s))
		goto error;

	if (alloc_kmem_cache_cpus(s))
		return 0;

	free_kmem_cache_nodes(s);
error:
	if (flags & SLAB_PANIC)
		panic("Cannot create slab %s size=%lu realsize=%u "
			"order=%u offset=%u flags=%lx\n",
			s->name, (unsigned long)s->size, s->size,
			oo_order(s->oo), s->offset, flags);
	return -EINVAL;
}
```

## 附录

### kmem_cache结构体

``` c++
struct kmem_cache {
	/* 指向包含空闲对象的本地高速缓存，每个CPU有一个该结构，当有对象释放是，优先放入本地CPU高速缓存中 */
	struct array_cache __percpu *cpu_cache;

/* 1) Cache tunables. Protected by slab_mutex */
/* 
 * 要一次性移进本地高速缓存或从本地高速缓存一次性移出去对象的数量 
 */
	unsigned int batchcount;
	/* 本地高速缓存中空闲对象的最大数目 */
	unsigned int limit;
	/* 是否存在CPU共享高速缓存，CPU共享高速缓存指针保存在kmem_cache_node结构中 */
	unsigned int shared;
	/* 对象长度 + 填充字节；相当于单个object内存大小 */
	unsigned int size;
	/* size的倒数，加快计算 */
	struct reciprocal_value reciprocal_buffer_size;
/* 2) touched by every alloc & free from the backend */
	/* 高速缓存永久属性的标识，如果SLAB描述符放在外部(不放在SLAB中)，则CFLAGS_OFF_SLAB置1 */
	unsigned int flags;		/* constant flags */
	/* 每个SLAB中对象的个数(在同一个高速缓存中slab中对象个数相同) */
	unsigned int num;		/* # of objs per slab */

/* 3) cache_grow/shrink */
	/* 一个单独SLAB中包含的连续页框数目的对数 */
	unsigned int gfporder;

	/* force GFP flags, e.g. GFP_DMA */
	/* 分配页框时传递给伙伴系统的一组标识 */
	gfp_t allocflags;
	/* SLAB使用的颜色个数 */
	size_t colour;			/* cache colouring range */
	/* SLAB中基本对齐偏移，当新SLAB着色时，偏移量的值需要乘上这个基本对齐偏移量，理解就是1个偏移量等于多少个B大小的值 */
	unsigned int colour_off;	/* colour offset */
	/* 空闲对象链表放在外部时使用，其指向的SLAB高速缓存来存储空闲对象链表 */
	struct kmem_cache *freelist_cache;
	/* 空闲对象链表的大小 */
	unsigned int freelist_size;

	/* 构造函数，一般用于初始化这个SLAB高速缓存中的对象 */
	void (*ctor)(void *obj);

/* 4) cache creation/removal */
	/* 存放高速缓存名字 */
	const char *name;
	/* 高速缓存描述符双向链表指针 */
	struct list_head list;
	/* 高速缓存被引用次数 */
	int refcount;
	/* 高速缓存中对象的大小 */
	int object_size;
	int align;

/* 5) statistics */
#ifdef CONFIG_DEBUG_SLAB
	unsigned long num_active;
	unsigned long num_allocations;
	unsigned long high_mark;
	unsigned long grown;
	unsigned long reaped;
	unsigned long errors;
	unsigned long max_freeable;
	unsigned long node_allocs;
	unsigned long node_frees;
	unsigned long node_overflow;
	atomic_t allochit;
	atomic_t allocmiss;
	atomic_t freehit;
	atomic_t freemiss;

	/*
	 * If debugging is enabled, then the allocator can add additional
	 * fields and/or padding to every object. size contains the total
	 * object size including these internal fields, the following two
	 * variables contain the offset to the user object and its size.
	 */
	int obj_offset;
#endif /* CONFIG_DEBUG_SLAB */
#ifdef CONFIG_MEMCG_KMEM
	struct memcg_cache_params memcg_params;
#endif
	/* 结点链表，此高速缓存可能在不同NUMA的结点都有SLAB链表 */
	struct kmem_cache_node *node[MAX_NUMNODES];
};
```

### kmem_cache_node结构体

``` c++
/* SLAB链表结构 */
struct kmem_cache_node {
    /* 锁 */
    spinlock_t list_lock;

/* SLAB用 */
#ifdef CONFIG_SLAB
    /* 只使用了部分对象的SLAB描述符的双向循环链表 */
    struct list_head slabs_partial;    /* partial list first, better asm code */
    /* 不包含空闲对象的SLAB描述符的双向循环链表 */
    struct list_head slabs_full;
    /* 只包含空闲对象的SLAB描述符的双向循环链表 */
    struct list_head slabs_free;
    /* 高速缓存中空闲对象个数(包括slabs_partial链表中和slabs_free链表中所有的空闲对象) */
    unsigned long free_objects;
    /* 高速缓存中空闲对象的上限 */
    unsigned int free_limit;
    /* 下一个被分配的SLAB使用的颜色 */
    unsigned int colour_next;    /* Per-node cache coloring */
    /* 指向这个结点上所有CPU共享的一个本地高速缓存 */
    struct array_cache *shared;    /* shared per node */
    struct alien_cache **alien;    /* on other nodes */
    /* 两次缓存收缩时的间隔，降低次数，提高性能 */
    unsigned long next_reap;    
    /* 0:收缩  1:获取一个对象 */
    int free_touched;        /* updated without locking */
#endif

/* SLUB用 */
#ifdef CONFIG_SLUB
    unsigned long nr_partial;
	/* 只使用了部分对象的SLAB描述符的双向循环链表 */
    struct list_head partial;
#ifdef CONFIG_SLUB_DEBUG
    atomic_long_t nr_slabs;
    atomic_long_t total_objects;
    struct list_head full;
#endif
#endif

};
```


## 参考资料

[slub debug](http://www.wowotech.net/memory_management/427.html)

[slub create代码解析](http://blog.chinaunix.net/uid-25940216-id-3202940.html)

[图解slub] (http://www.wowotech.net/memory_management/426.html)

[slub 分配器描述](https://www.cnblogs.com/tolimit/p/4566189.html#)
